package cn.ac.ioa.hccl.libsp.util;
import java.io.*;
import java.util.*;

/**
 * One dimensional GMM.
 * @author nay0648
 */
public class GaussianMixtureModel1D implements Serializable
{
private static final long serialVersionUID = 2512246393274161541L;
private static final double EPS=2.220446049250313e-16;//a small positive number
private static final Random RAND=new Random();//used to generate Gaussian random number
private int maxitkmeans=100;//the maximum number of iterations for the k-means
private int maxitem=200;//the maximum number of iterations for the em
private double stopthkmeans=1e-10;//the stop threshold of the k-means
private double stopthem=1e-8;//the stop threshold of the em algorithm
private double[] weight;//gaussian weight
private Gaussian[] gaussians;
private double[] tempresp;//used to store responsibility
private long numsamples=0;//number of samples trained by the model

	/**
	 * @param numgaussians
	 * number of gaussians
	 */
	public GaussianMixtureModel1D(int numgaussians)
	{
		weight=new double[numgaussians];
		Arrays.fill(weight, 1.0/weight.length);
		
		gaussians=new Gaussian[numgaussians];
		for(int i=0;i<gaussians.length;i++) gaussians[i]=new Gaussian(0,1);
		
		tempresp=new double[numgaussians];
	}
	
	/**
	 * get the number of gaussians
	 * @return
	 */
	public int numGaussians()
	{
		return gaussians.length;
	}
	
	/**
	 * get a Gaussian function
	 * @param gidx
	 * gaussian index
	 * @return
	 */
	public Gaussian gaussian(int gidx)
	{
		return gaussians[gidx];
	}

	/**
	 * get the weight of a gaussian
	 * @param gidx
	 * gaussian index
	 * @return
	 */
	public double getWeight(int gidx)
	{
		return weight[gidx];
	}
	
	/**
	 * set the weight of a gaussian
	 * @param gidx
	 * gaussian index
	 * @param w
	 * the weight
	 */
	public void setWeight(int gidx,double w)
	{
		weight[gidx]=w;
	}
	
	/**
	 * select a Gaussian according to the underlying weight
	 * @return
	 * index of Gaussian
	 */
	public int selectGaussian()
	{
	double r,sum=0;
	int gidx;
	
		r=RAND.nextDouble();
		
		for(gidx=0;gidx<weight.length;gidx++) 
		{
			sum+=weight[gidx];
			if(r<sum) break;
		}
		
		if(gidx>numGaussians()-1) gidx=numGaussians()-1;
		return gidx;
	}
	
	/**
	 * calculate the probability density the GMM generates x
	 * @param x
	 * a variable
	 * @return
	 */
	public double probabilityDensity(double x)
	{
	double px=0;
	
		for(int i=0;i<weight.length;i++) 
			if(gaussians[i]!=null) 
				px+=weight[i]*gaussians[i].probabilityDensity(x);
		
		return px;
	}
	
	/**
	 * get a random variable generated by the Gaussian mixture model
	 * @return
	 */
	public double nextSample()
	{
	Gaussian g;
	
		g=gaussians[selectGaussian()];
		return RAND.nextGaussian()*g.getStandardDeviation()+g.getMean();
	}
	
	/**
	 * randomly select initial centers
	 * @param dataset
	 * the dataset
	 * @param centers
	 * space for initial centers
	 */
	public void initCentersRandom(int[] dataset,double[] centers)
	{
	int retry=10;
	Set<Double> cs;
	double c=0;
	
		cs=new HashSet<Double>();
		
		for(int i=0;i<centers.length;i++) 
		{
			for(int count=0;count<retry;count++) 
			{
				//randomly select a sample
				c=dataset[(int)(RAND.nextDouble()*dataset.length)];
				//this sample is not used
				if(!cs.contains(c)) 
				{
					cs.add(c);
					break;
				}
			}
			
			centers[i]=c;
		}
	}
	
	/**
	 * randomly select initial centers
	 * @param dataset
	 * the dataset
	 * @param centers
	 * space for initial centers
	 */
	public void initCentersRandom(double[] dataset,double[] centers)
	{
	int retry=10;
	Set<Double> cs;
	double c=0;
	
		cs=new HashSet<Double>();
		
		for(int i=0;i<centers.length;i++) 
		{
			for(int count=0;count<retry;count++) 
			{
				//randomly select a sample
				c=dataset[(int)(RAND.nextDouble()*dataset.length)];
				//this sample is not used
				if(!cs.contains(c)) 
				{
					cs.add(c);
					break;
				}
			}
			
			centers[i]=c;
		}
	}
	
	/**
	 * the k-means clustering algorithm
	 * @param dataset
	 * dataset 
	 * @param kinit
	 * k initial centers, cluster centers will also be returned from here
	 * @param indicator
	 * clustering result, indicator index starts from 0
	 * @return
	 * The tolerance, i.e. average distance from each sample to corresponding 
	 * clustering center, then average again for each cluster. Return -1 if 
	 * failed to cluster.
	 */
	public double kmeans(int[] dataset,double[] kinit,int[] indicator)
	{
	double d,mind,var;
	int[] count;//number of samples belong to each cluster
	double[] tolerance;//average distance from each sample to its center for every center
	double[] kcenter2;
	
		if(dataset.length!=indicator.length) throw new IllegalArgumentException(
				"incompatible indicator length: "+indicator.length+", required: "+dataset.length);
		
		/*
		 * perform the clustering
		 */
		count=new int[kinit.length];
		tolerance=new double[kinit.length];
		//new cluster centers
		kcenter2=new double[kinit.length];
		
		for(int itcount=0;itcount<maxitkmeans;itcount++)
		{
			Arrays.fill(count,0);//clear cluster count
			Arrays.fill(tolerance,0);//clear tolerance
			Arrays.fill(kcenter2,0);//clear new cluster center
			
			//traverse dataset
			for(int i=0;i<dataset.length;i++)
			{
				/*
				 * assign samples to clusters
				 */
				mind=Double.MAX_VALUE;
				//calculate distance from a sample to all cluster centers
				for(int ii=0;ii<kinit.length;ii++)
				{
					//distance from a sample to a cluster center
					d=Math.abs(dataset[i]-kinit[ii]);
					//find a smaller distance
					if(d<mind)
					{
						mind=d;
						indicator[i]=ii;
					}
				}
				
				/*
				 * here indicator[i] is the sample i's cluster label
				 */
				count[indicator[i]]++;//count the sample belongs to the cluster
				tolerance[indicator[i]]+=mind;//accumulate tolerance of a cluster
				
				//accumulate the new cluster center					
				kcenter2[indicator[i]]+=dataset[i];
			}
			
			//average tolerance for each cluster
			for(int i=0;i<tolerance.length;i++) tolerance[i]/=count[i];
			//calculate new cluster centers
			for(int i=0;i<kcenter2.length;i++) kcenter2[i]/=count[i];
			
			/*
			 * calculate average cluster centers displacement
			 */
			var=0;
			for(int i=0;i<kinit.length;i++) var+=Math.abs(kinit[i]-kcenter2[i]);
			var/=kinit.length;
			
			//copy cluster centers
			System.arraycopy(kcenter2, 0, kinit, 0, kinit.length);
			
			//stop condition: cluster centers become stable
			if(var<=stopthkmeans) return BLAS.mean(tolerance);
		}
		
		return -1;
	}
	
	/**
	 * the k-means clustering algorithm
	 * @param dataset
	 * dataset 
	 * @param kinit
	 * k initial centers, cluster centers will also be returned from here
	 * @param indicator
	 * clustering result, indicator index starts from 0
	 * @return
	 * The tolerance, i.e. average distance from each sample to corresponding 
	 * clustering center, then average again for each cluster. Return -1 if 
	 * failed to cluster.
	 */
	public double kmeans(double[] dataset,double[] kinit,int[] indicator)
	{
	double d,mind,var;
	int[] count;//number of samples belong to each cluster
	double[] tolerance;//average distance from each sample to its center for every center
	double[] kcenter2;
	
		if(dataset.length!=indicator.length) throw new IllegalArgumentException(
				"incompatible indicator length: "+indicator.length+", required: "+dataset.length);
		
		/*
		 * perform the clustering
		 */
		count=new int[kinit.length];
		tolerance=new double[kinit.length];
		//new cluster centers
		kcenter2=new double[kinit.length];
		
		for(int itcount=0;itcount<maxitkmeans;itcount++)
		{
			Arrays.fill(count,0);//clear cluster count
			Arrays.fill(tolerance,0);//clear tolerance
			Arrays.fill(kcenter2,0);//clear new cluster center
			
			//traverse dataset
			for(int i=0;i<dataset.length;i++)
			{
				/*
				 * assign samples to clusters
				 */
				mind=Double.MAX_VALUE;
				//calculate distance from a sample to all cluster centers
				for(int ii=0;ii<kinit.length;ii++)
				{
					//distance from a sample to a cluster center
					d=Math.abs(dataset[i]-kinit[ii]);
					//find a smaller distance
					if(d<mind)
					{
						mind=d;
						indicator[i]=ii;
					}
				}
				
				/*
				 * here indicator[i] is the sample i's cluster label
				 */
				count[indicator[i]]++;//count the sample belongs to the cluster
				tolerance[indicator[i]]+=mind;//accumulate tolerance of a cluster
				
				//accumulate the new cluster center					
				kcenter2[indicator[i]]+=dataset[i];
			}
			
			//average tolerance for each cluster
			for(int i=0;i<tolerance.length;i++) tolerance[i]/=count[i];
			//calculate new cluster centers
			for(int i=0;i<kcenter2.length;i++) kcenter2[i]/=count[i];
			
			/*
			 * calculate average cluster centers displacement
			 */
			var=0;
			for(int i=0;i<kinit.length;i++) var+=Math.abs(kinit[i]-kcenter2[i]);
			var/=kinit.length;
			
			//copy cluster centers
			System.arraycopy(kcenter2, 0, kinit, 0, kinit.length);
			
			//stop condition: cluster centers become stable
			if(var<=stopthkmeans) return BLAS.mean(tolerance);
		}
		
		return -1;
	}
	
	/**
	 * calculate the responsibility which a sample is generated by each Gaussian
	 * @param x
	 * a sample
	 * @param resp
	 * space for responsibility, null to allocate new space
	 * @return
	 * responsibility for each Gaussian
	 */
	public double[] responsibility(double x,double[] resp)
	{
	double p;
	
		if(resp==null) resp=new double[numGaussians()];
		else BLAS.checkDestinationSize(resp,numGaussians());

		System.arraycopy(weight,0,resp,0,resp.length);//probability of each Gaussian
	
		for(int gidx=0;gidx<resp.length;gidx++) 
		{
			p=gaussians[gidx].probabilityDensity(x);
			
			if(p<EPS) resp[gidx]*=EPS;
			else resp[gidx]*=p;
		}
		
		//normalize as stochastic vector
		BLAS.scalarMultiply(1.0/BLAS.sum(resp),resp,resp);
	
		return resp;
	}
	
	/**
	 * calculate the log-likelihood which the model generates the dataset
	 * @param dataset
	 * a dataset
	 * @return
	 */
	public double logLikelihood(int[] dataset)
	{
	double p,logp=0;
	
		for(int i=0;i<dataset.length;i++) 
		{
			p=probabilityDensity(dataset[i]);
			
			if(p<EPS) logp+=Math.log(EPS);
			else logp+=Math.log(p);
		}
		
		return logp;
	}
	
	/**
	 * calculate the log-likelihood which the model generates the dataset
	 * @param dataset
	 * a dataset
	 * @return
	 */
	public double logLikelihood(double[] dataset)
	{
	double p,logp=0;
	
		for(int i=0;i<dataset.length;i++) 
		{
			p=probabilityDensity(dataset[i]);
			
			if(p<EPS) logp+=Math.log(EPS);
			else logp+=Math.log(p);
		}
		
		return logp;
	}
	
	/**
	 * train the model
	 * @param dataset
	 * a dataset
	 * @param kmeansinit
	 * true to use k-means to initialize the model
	 * @return
	 * log-likelihood
	 */
	public double trainModel(int[] dataset,boolean kmeansinit)
	{
		//initialize with k-means
		if(kmeansinit) 
		{
		double[] centers;
		int[] indicator;
		double tol;
		double[] sigma;
		double temp;
		
			centers=new double[numGaussians()];
			indicator=new int[dataset.length];
			
			/*
			 * perform k-means
			 */
			initCentersRandom(dataset, centers);
			tol=kmeans(dataset, centers, indicator);
			
			//initialize gmm by k-means results
			if(tol>0) 
			{
				sigma=new double[numGaussians()];
				Arrays.fill(weight, 0);
				
				for(int i=0;i<dataset.length;i++) 
				{
					temp=dataset[i]-centers[indicator[i]];
					sigma[indicator[i]]+=temp*temp;
					weight[indicator[i]]++;
				}

				for(int i=0;i<sigma.length;i++) 
				{
					sigma[i]=Math.sqrt(sigma[i]/weight[i]);
					
					if(sigma[i]==0) gaussians[i]=new Gaussian(centers[i],1);
					else gaussians[i]=new Gaussian(centers[i],sigma[i]);
				}
				BLAS.scalarMultiply(1.0/BLAS.sum(weight), weight, weight);
			}
			else
			{
				initCentersRandom(dataset, centers);
				for(int i=0;i<gaussians.length;i++) 
					gaussians[i]=new Gaussian(centers[i],1);
			}
		}
			
		//the em algorithm
		{
		double logp,logp2;	
		double[][] resp;//each row is for a sample
		double mu2,sigma2,temp;
		
			resp=new double[dataset.length][numGaussians()];
			logp=logLikelihood(dataset);
			
			for(int itcount=0;itcount<maxitem;itcount++) 
			{
				//E-step
				for(int sidx=0;sidx<dataset.length;sidx++) 
					responsibility(dataset[sidx], resp[sidx]);
				
				/*
				 * M-step
				 */
				for(int gidx=0;gidx<numGaussians();gidx++) 
				{
					weight[gidx]=0;
					mu2=0;
					sigma2=0;

					/*
					 * update mean and weight
					 */
					for(int sidx=0;sidx<dataset.length;sidx++) 
					{
						weight[gidx]+=resp[sidx][gidx];
						mu2+=resp[sidx][gidx]*dataset[sidx];
					}
					mu2/=weight[gidx];
					gaussians[gidx].setMean(mu2);
					
					/*
					 * update standard deviation
					 */
					for(int sidx=0;sidx<dataset.length;sidx++) 
					{
						temp=dataset[sidx]-mu2;
						sigma2+=resp[sidx][gidx]*temp*temp;
					}
					gaussians[gidx].setStandardDeviation(Math.sqrt(sigma2/weight[gidx]));
				}
				
				//normalize the weight
				BLAS.scalarMultiply(1.0/BLAS.sum(weight),weight,weight);
				
				/*
				 * to see if the algorithm converged
				 */
				logp2=logLikelihood(dataset);
				if(Math.abs(logp2-logp)<=stopthem) 
				{
					numsamples+=dataset.length;
					return logp2;
				}
				else logp=logp2;
			}
			
			//not converge
			return -1;
		}
	}
	
	/**
	 * train the model
	 * @param dataset
	 * a dataset
	 * @param kmeansinit
	 * true to use k-means to initialize the model
	 * @return
	 * log-likelihood
	 */
	public double trainModel(double[] dataset,boolean kmeansinit)
	{
		//initialize with k-means
		if(kmeansinit) 
		{
		double[] centers;
		int[] indicator;
		double tol;
		double[] sigma;
		double temp;
		
			centers=new double[numGaussians()];
			indicator=new int[dataset.length];
			
			/*
			 * perform k-means
			 */
			initCentersRandom(dataset, centers);
			tol=kmeans(dataset, centers, indicator);
			
			//initialize gmm by k-means results
			if(tol>0) 
			{
				sigma=new double[numGaussians()];
				Arrays.fill(weight, 0);
				
				for(int i=0;i<dataset.length;i++) 
				{
					temp=dataset[i]-centers[indicator[i]];
					sigma[indicator[i]]+=temp*temp;
					weight[indicator[i]]++;
				}

				for(int i=0;i<sigma.length;i++) 
				{
					sigma[i]=Math.sqrt(sigma[i]/weight[i]);
					
					if(sigma[i]==0) gaussians[i]=new Gaussian(centers[i],1);
					else gaussians[i]=new Gaussian(centers[i],sigma[i]);
				}
				BLAS.scalarMultiply(1.0/BLAS.sum(weight), weight, weight);
			}
			else
			{
				initCentersRandom(dataset, centers);
				for(int i=0;i<gaussians.length;i++) 
					gaussians[i]=new Gaussian(centers[i],1);
			}
		}
			
		//the em algorithm
		{
		double logp,logp2;	
		double[][] resp;//each row is for a sample
		double mu2,sigma2,temp;
		
			resp=new double[dataset.length][numGaussians()];
			logp=logLikelihood(dataset);
			
			for(int itcount=0;itcount<maxitem;itcount++) 
			{
				//E-step
				for(int sidx=0;sidx<dataset.length;sidx++) 
					responsibility(dataset[sidx], resp[sidx]);
				
				/*
				 * M-step
				 */
				for(int gidx=0;gidx<numGaussians();gidx++) 
				{
					weight[gidx]=0;
					mu2=0;
					sigma2=0;

					/*
					 * update mean and weight
					 */
					for(int sidx=0;sidx<dataset.length;sidx++) 
					{
						weight[gidx]+=resp[sidx][gidx];
						mu2+=resp[sidx][gidx]*dataset[sidx];
					}
					mu2/=weight[gidx];
					gaussians[gidx].setMean(mu2);
					
					/*
					 * update standard deviation
					 */
					for(int sidx=0;sidx<dataset.length;sidx++) 
					{
						temp=dataset[sidx]-mu2;
						sigma2+=resp[sidx][gidx]*temp*temp;
					}
					gaussians[gidx].setStandardDeviation(Math.sqrt(sigma2/weight[gidx]));
				}
				
				//normalize the weight
				BLAS.scalarMultiply(1.0/BLAS.sum(weight),weight,weight);
				
				/*
				 * to see if the algorithm converged
				 */
				logp2=logLikelihood(dataset);
				if(Math.abs(logp2-logp)<=stopthem) 
				{
					numsamples+=dataset.length;
					return logp2;
				}
				else logp=logp2;
			}
			
			//not converge
			return -1;
		}
	}
	
	/**
	 * train the model online
	 * @param sample
	 * one data sample
	 */
	public void trainModel(double sample)
	{
		//the E-step
		responsibility(sample, tempresp);
		
		//the M-step
		{
		double w,mu,sigma,temp;
		Gaussian g;
		
			for(int gidx=0;gidx<numGaussians();gidx++) 
			{
				g=gaussians[gidx];
				
				/*
				 * turn back to the summed value
				 */
				w=weight[gidx]*numsamples;
				mu=g.getMean()*w;
				sigma=g.getStandardDeviation()*g.getStandardDeviation()*w;
				
				/*
				 * accumulate new sample
				 */
				w+=tempresp[gidx];
				
				mu+=tempresp[gidx]*sample;
				mu/=w;
				
				temp=sample-mu;
				sigma+=tempresp[gidx]*temp*temp;
				sigma=Math.sqrt(sigma/w);
				
				/*
				 * set new parameters
				 */
				g.setMean(mu);
				g.setStandardDeviation(sigma);
				weight[gidx]=w;
			}
			
			BLAS.scalarMultiply(1.0/BLAS.sum(weight), weight, weight);
			numsamples++;
		}
	}
	
	public String toString()
	{
	StringBuilder s;
	
		s=new StringBuilder();
		
		for(int i=0;i<numGaussians();i++) 
			s.append(weight[i]+", "+gaussians[i]+"\n");
		
		return s.toString();
	}
	
	/**
	 * One dimensional Gaussian function.
	 * @author nay0648
	 */
	public class Gaussian implements Serializable
	{
	private static final long serialVersionUID = 3759042623649548741L;
	private double sqrt2pi=Math.sqrt(2*Math.PI);
	private double mu;//mean
	private double sigma;//standard deviation
	
		/**
		 * @param mu
		 * the mean
		 * @param sigma
		 * the standard deviation
		 */
		public Gaussian(double mu,double sigma)
		{
			this.mu=mu;
			this.sigma=sigma;
		}
		
		/**
		 * get the mean
		 * @return
		 */
		public double getMean()
		{
			return mu;
		}
		
		/**
		 * set the mean
		 * @param mu
		 * mean
		 */
		public void setMean(double mu)
		{
			this.mu=mu;
		}
		
		/**
		 * get the standard deviation
		 * @return
		 */
		public double getStandardDeviation()
		{
			return sigma;
		}
		
		/**
		 * set the standard deviation
		 * @param sigma
		 * the standard deviation
		 */
		public void setStandardDeviation(double sigma)
		{
			this.sigma=sigma;
		}
		
		/**
		 * calculate the probability density
		 * @param x
		 * the variable
		 * @return
		 */
		public double probabilityDensity(double x)
		{
		double temp,p;
		
			temp=(x-mu)/sigma;
			p=Math.exp(-temp*temp/2.0)/(sqrt2pi*sigma);
			
			if(Double.isNaN(p)) return 0;
			else return p;
		}
		
		public String toString()
		{
			return "["+mu+", "+sigma+"]";
		}
	}
	
	public static void main(String[] args)
	{
//	int[] x={0, 182, 198, 196, 205, 206, 216, 222, 227, 230, 232, 228, 230, 222, 169, 162, 220, 233, 232, 233, 164, 219, 219, 233, 233, 229, 229, 219, 230, 234};
	int[] x={182, 195, 192, 202, 203, 211, 216, 220, 223, 225, 221, 235, 233, 187, 171, 173, 163, 163, 223, 221, 217, 217, 165, 165, 158, 158, 191, 185, 230, 184, 184, 156, 155, 155, 188, 232, 232, 221, 162, 225, 222, 159, 222, 161, 230, 230, 234, 234, 234};	
	double[] pdf;
	GaussianMixtureModel1D gmm;
	
		gmm=new GaussianMixtureModel1D(3);
		
		gmm.trainModel(x,true);
		
		for(int i=0;i<10;i++) 
			for(int j=0;j<x.length;j++) gmm.trainModel(x[j]);
		
		pdf=new double[361];
		for(int i=0;i<pdf.length;i++) pdf[i]=gmm.probabilityDensity(i);
		Util.plotSignals(pdf);
	}
}
